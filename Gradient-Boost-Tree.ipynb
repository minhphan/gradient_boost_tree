{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content\n",
    "1. Bagging & Boosting.\n",
    "2. Gradient Boosting Tree in Sklearn's implementation with note from **Jerome H Friedman, Greedy function approximation:  A gradient boosting machine, 1999** \n",
    "3. Gradient Boosting Tree's implementation from scratch.\n",
    "\n",
    "### After this, you can...\n",
    "1. Understand partially theory and coding structure of Gradient Boosting Tree.\n",
    "2. Contribute your knowledge to this post."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Brief Introduction of Tree Ensembles \n",
    "\n",
    "* Bagging\n",
    "    * Create classifers using training sets that are bootstraped (drawn with replacement) \n",
    "    * Average results for each case.\n",
    "\n",
    "* Boosting \n",
    "    * Sequential production of classifers\n",
    "    * Each classifer tries to fix previous one's errors\n",
    "\n",
    "Similarity\n",
    "> The predictions from all of them are then combined through a **weighted majority vote** to produce the final prediction.\n",
    "\n",
    "Difference\n",
    "> A weak classifier is one whose error rate is only slightly better than random guessing. The purpose of boosting is to **sequentially apply the weak classification algorithm to repeatedly modified versions of the data**, thereby producing a sequence of weak classifiers.\n",
    "\n",
    "\n",
    "[](\\begin{equation} \n",
    "    G(x) = \\text{sign}\\Big(\\sum^M_{m=1}\\alpha_mG_m(x) \\Big). \n",
    "\\end{equation})\n",
    "\n",
    "### Bagging\n",
    "\n",
    "> Each bootstrap tree will typically involve different features than the original, and might have a different number of terminal nodes. The bagged estimate is the average prediction at $x$ from these $B$ trees.\n",
    "<img src=\"img/8-9.png\" width=\"400\" height=\"400\"> \n",
    "\n",
    "> The trees have high variance due to the correlation in the predictors. Bagging succeeds in smoothing out this variance and hence reducing the test error. Error curves for the bagging example of Figure 8.9. Shown is the test error of the original tree and bagged trees as a function of the number of bootstrap samples. The orange points correspond to the consensus vote, while the green points average the probabilities.\n",
    "<img src=\"img/8-10.png\" width=\"400\" height=\"400\">\n",
    "\n",
    "#### Bagging's Intuition\n",
    "> The collective knowledge of a diverse and independent body of people typically exceeds the knowledge of any single individual, and can be harnessed by voting. Of course, the main caveat here is “independent,” and bagged trees are not. \n",
    "\n",
    "> Note that when we bag a model, any simple structure in the model is lost. As an example, a bagged tree is no longer a tree. For interpretation of the model this is clearly a drawback.\n",
    "\n",
    "* Bagging is a bunch of dependent weak learners who try to make better prediction than an individual.\n",
    "> Bagging estimates the expected class probabilities from the single split rule, that is, **averaged over many replications**. Note that the expected class probabilities computed by bagging cannot be realized on any single replication, in the same way that a woman cannot have 2.4 children. In this sense, bagging increases somewhat the space of models of the individual base classifier.\n",
    "<img src=\"img/8-12.png\" width=\"400\" height=\"400\"> \n",
    "\n",
    "[](\\begin{equation} \n",
    "    \\hat{f}_{\\text{bag}}(x) = \\frac{1}{B}\\sum^B_{b=1} \\hat{f}^{ \\ast b}(x) (6.13) \n",
    "\\end{equation})\n",
    "\n",
    "### Boosting\n",
    "A tree can be expressed as:\n",
    "\\begin{equation} \n",
    "    T(x;\\Theta) = \\sum^J_{j=1}\\gamma_j I(x \\in R_j)\n",
    "\\end{equation}\n",
    "\n",
    "The boosted tree model is a sum of such trees,\n",
    "\\begin{equation} \n",
    "    f_M(x) = \\sum^M_{m=1} T(x,\\Theta_m),\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting \n",
    "Given a training data $\\{\\mathbf{x_i},y_i\\}^N_{i=1}$, we need to find a model $F(\\mathbf{x})$ that map $\\mathbf{x}$ to $y$. We will measure the mapping effectiveness by choosing a loss function $L(y,F(\\mathbf{x}))$ and then minimize it:\n",
    "\n",
    "\\begin{equation} \\label{eq.6.13}\n",
    "    F^{\\ast} = \\text{argmin}_F E_{y,\\mathbf{x}}L(y,F(\\mathbf{x})) (6.13) \n",
    "\\end{equation}\n",
    "\n",
    "The model $F(\\mathbf{x})$ is constructed in the form:\n",
    "\n",
    "\\begin{equation} \\label{eq.6.14b}\n",
    "    F(\\mathbf{x}) = F_0(\\mathbf{x}) + \\sum^M_{m=1} \\rho_m h_m(\\mathbf{x}) (6.15)\n",
    "\\end{equation} \n",
    "\n",
    "This is called boosting where:\n",
    "* $F_0(\\mathbf{x})$ is a initial guess, the first starting point of the model.\n",
    "* $h_m(\\mathbf{x})$ is called \"weak learner\" or \"base learner\".\n",
    "\n",
    "We start the process by choosing $F_0(\\mathbf{x})$, then measure the error between target $y$ and prediction $F_0(\\mathbf{x})$: $y - F_0(\\mathbf{x})$. Next, we try to find a new function $h_0(\\mathbf{x})$ so that:\n",
    "\\begin{equation*} \n",
    "    F_0(\\mathbf{x}) + \\rho_0 h_0(\\mathbf{x}) = y\n",
    "\\end{equation*}\n",
    "\n",
    "The role of $h_0$ is to reduce the error of model $F(\\mathbf{x})$. This step is proceeded recursively, we can add up another $h_1,\\dots,h_m$ until the best $F^\\ast$ is found. Each new $h_m$ will try to correct errors made by previous $h_{m-1}$ so that:\n",
    "\n",
    "\\begin{equation}\n",
    "    F_m(\\mathbf{x}) + \\rho_m h_m(\\mathbf{x}) = y.\n",
    "\\end{equation}\n",
    "\n",
    "One way to obtain that is to use \"steepest descent\" to find the right direction for $h_m$ to follow:\n",
    "\n",
    "\\begin{equation} \\label{eq.6.16}\n",
    "    g_{m}(\\mathbf{x_i}) = - \\Big[\\frac{\\partial L\\big(y_i,F(\\mathbf{x_i})\\big)}{\\partial F(\\mathbf{x_i})} \\Big]_{F(\\mathbf{x}) = F_{m-1}(\\mathbf{x}) }, i = 1,\\dots, N   (6.16)\n",
    "\\end{equation} \n",
    "\n",
    "The negative gradient $-g_{m}(\\mathbf{x_i})$ is called the “linear search” along that direction in order for $\\rho_mh_m$ to obtain “best greedy step” towards $F^\\ast(\\mathbf{x})$ in (6.15). We know that $\\rho_mh_m$ has to output $g_{m}(\\mathbf{x_i})$ allowing itself to “step” exactly in the direction of the negative gradient. Using a “rough” solution by only fitting $h_m$ into the training data $\\{\\mathbf{x_i},g_{m}(\\mathbf{x_i})\\}^N_{i=1}:$\n",
    "\n",
    "\n",
    "\\begin{equation} \\label{eq.6.17}\n",
    "    h_m = \\text{arg min}_h\\sum^N_{i=1}[-g_{m}(\\mathbf{x_i}) - h(\\mathbf{x_i})]^2 (6.17)\n",
    "\\end{equation} \n",
    "\n",
    "The step size is optimized through linear search that satisfies:\n",
    "\n",
    "\\begin{equation} \\label{eq.6.18}\n",
    "    \\rho_m = \\text{arg min}_{\\rho} \\sum^N_{i=1} L\\big(y_i,F_{m-1}(\\mathbf{x}) + \\rho h_m(\\mathbf{x})\\big).\n",
    "\\end{equation}\n",
    "\n",
    "From 6.17 we can see that $h_m(\\mathbf{x}) \\sim g_m(\\mathbf{x})$, using 6.16 we examine 6.15 under the form:\n",
    "\n",
    "\\begin{equation} \\label{eq.6.19}\n",
    "    F(\\mathbf{x}) \\sim F_0(\\mathbf{x}) - \\rho_m \\Big[\\frac{\\partial L\\big(y_i,F(\\mathbf{x_i})\\big)}{\\partial F(\\mathbf{x_i})} \\Big]_{F(\\mathbf{x}) = F_{m-1}(\\mathbf{x}) }. (6.19)\n",
    "\\end{equation} \n",
    "\n",
    "How can new $h_m$  correct errors made by prior $h_{m-1}$? From 6.19 we can see what it does is similarly to Gradient Descent. At each iteration, $h_m $ will continue to explore the path that its predecessor $h_{m-1}$ has already followed and try to reach closer to the local minimum of loss function by following the negative gradient direction using the “steepest descent”. The difference between Gradient Boosting and Gradient Descent in each step is: Gradient Boosting adds **a new function to the model** that moves along the negative gradient direction. Whereas Gradient Descent **update its parameter** along the negative gradient direction. Both also try to reach a local minimum of the loss function.\n",
    "\n",
    "### Tree Boost\n",
    "Each learner is an $J$-terminal node regression tree. Indicator $\\mathbf{1}$ has value of 1 if its argument is true, 0 otherwise. $J$ is the number of its leaves. $\\{R_{j}\\}^{J}_{1}$ are disjoint regions correspond to the terminal nodes or leaves of the tree with it's predicted value $\\{b_{j}\\}^{J}_{1}$, if $\\mathbf{x} \\in R_j$ then $h(\\mathbf{x}) = b_j$.\n",
    "\n",
    "\\begin{equation} \n",
    "    h(\\mathbf{x};\\{b_j, R_j\\}^J_1) = \\sum^J_{j=1}b_j\\mathbf{1}(\\mathbf{x} \\in R_j).\n",
    "\\end{equation} \n",
    "\n",
    "For a regression tree, 6.15 becomes:\n",
    "\\begin{equation} \n",
    "        F_m(\\mathbf{x}) = F_{m-1}(\\mathbf{x}) + \\rho_m \\sum^J_{j=1}b_{jm}\\mathbf{1}(\\mathbf{x} \\in R_{jm}).\n",
    "\\end{equation} \n",
    "\n",
    "$\\{R_{jm}\\}^J_1$ are regions at $m$th iteration. The ${b_{jm}}$ are the corresponding least-squares coefficients:\n",
    "\\begin{equation} \n",
    "    b_{jm} = \\text{mean}_{\\mathbf{x}_i \\in R_{jm}} \\tilde{y}_i.\n",
    "\\end{equation} \n",
    "\n",
    "With $\\gamma_{jm} = \\rho_m b_{jm}$\n",
    "\\begin{equation} \n",
    "    F_m(\\mathbf{x}) = F_{m-1}(\\mathbf{x}) + \\sum^J_{j=1}\\gamma_{jm}\\mathbf{1}(\\mathbf{x} \\in R_{jm}).\n",
    "\\end{equation} \n",
    "\n",
    "Optimal coefficients are the the solution to\n",
    "\\begin{equation} \n",
    "    \\{\\gamma_{jm}\\}^J_1 = \\text{arg min}_{\\{\\gamma_{j}\\}^J_1} \\sum^N_{i=1}L \\Bigg(y_i, F_{m-1}(\\mathbf{x}_i) + \\sum^J_{j=1}\\gamma_{j}\\mathbf{1}(\\mathbf{x} \\in R_{jm}) \\Bigg).\n",
    "\\end{equation} \n",
    "\n",
    "Owning to the disjoint nature of the regions produced by regression trees, this reduce to\n",
    "\\begin{equation} \n",
    "    \\gamma_{jm} = \\text{arg min}_{\\gamma} \\sum_{\\mathbf{x}_i \\in R_{jm}}L (y_i, F_{m-1}(\\mathbf{x}_i) + \\gamma).\n",
    "\\end{equation} \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GradientBoostingClassifier implementation in Sklearn\n",
    "\n",
    "* Init base learner $h_0(\\mathbf{x})$.\n",
    "* Init `estimators_` to store prediction of each learner for each feature (class), `shape = (n_estimators, n_classes)`.\n",
    "* Init `train_score_` to store prediction after each loop, `shape = (n_estimators,)`\n",
    "* Decide which loss function to use.\n",
    "\n",
    "Check Section 4. in [1] to see details of all loss functions.\n",
    "\n",
    "1. Binary classification: BinomialDeviance\n",
    "2. M-Regression: Huber\n",
    "3. Gradient Boosting Regression: Quantile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientBoostingClassifier(BaseGradientBoosting, ClassifierMixin):\n",
    "    \"\"\"Gradient Boosting for classification.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    loss : {'deviance', 'exponential'}, optional (default='deviance')\n",
    "        loss function to be optimized. 'deviance' refers to\n",
    "        deviance (= logistic regression) for classification\n",
    "        with probabilistic outputs. For loss 'exponential' gradient\n",
    "        boosting recovers the AdaBoost algorithm. \n",
    "    \"\"\"\n",
    "\n",
    "class BaseGradientBoosting(six.with_metaclass(ABCMeta, BaseEnsemble)):\n",
    "    def fit(self, X, y, sample_weight=None, monitor=None):\n",
    "        self._check_params()        \n",
    "        if not self._is_initialized():\n",
    "            # init state\n",
    "            self._init_state()\n",
    "            \n",
    "            \"\"\"\n",
    "            These will invoke methods in class `LogOddsEstimator`\n",
    "            * `self.init_.fit` will calculate h(x)\n",
    "            * `self.init_.predict` will calculate prediction of h(x)\n",
    "            \"\"\"    \n",
    "            # fit initial model - FIXME make sample_weight optional\n",
    "            self.init_.fit(X, y, sample_weight)\n",
    "\n",
    "            # init predictions\n",
    "            y_pred = self.init_.predict(X)\n",
    "            \n",
    "    def _check_params(self):\n",
    "        if (self.loss not in self._SUPPORTED_LOSS\n",
    "                or self.loss not in LOSS_FUNCTIONS):\n",
    "            raise ValueError(\"Loss '{0:s}' not supported. \".format(self.loss))\n",
    "\n",
    "        if self.loss == 'deviance':\n",
    "            loss_class = (MultinomialDeviance\n",
    "                          if len(self.classes_) > 2\n",
    "                          else BinomialDeviance)\n",
    "        else:\n",
    "            loss_class = LOSS_FUNCTIONS[self.loss]\n",
    "\n",
    "        if self.loss in ('huber', 'quantile'):\n",
    "            self.loss_ = loss_class(self.n_classes_, self.alpha)\n",
    "        else:\n",
    "            self.loss_ = loss_class(self.n_classes_)\n",
    "    \n",
    "    def _init_state(self):\n",
    "        \"\"\"Initialize model state and allocate model state data structures. \"\"\"\n",
    "        if self.init is None:\n",
    "        \"\"\"\n",
    "        This will invoke `LossFunction` to compute forward the learner.\n",
    "        \"\"\"\n",
    "            self.init_ = self.loss_.init_estimator()\n",
    "        elif isinstance(self.init, six.string_types):\n",
    "            self.init_ = INIT_ESTIMATORS[self.init]()\n",
    "        else:\n",
    "            self.init_ = self.init\n",
    "\n",
    "        self.estimators_ = np.empty((self.n_estimators, self.loss_.K),\n",
    "                                    dtype=np.object)\n",
    "        self.train_score_ = np.zeros((self.n_estimators,), dtype=np.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Calculate forward pass of learner $h_0(\\mathbf{x})$, this will be calculated only __1 time__.\n",
    "\n",
    "`LogOddsEstimator` will calculate $F_0(\\mathbf{x})$\n",
    "\n",
    "\\begin{equation}\n",
    "F(\\mathbf{x}) = \\frac{1}{2} \\text{log} \\Big[ \\frac{\\text{Pr}(y=1|\\mathbf{x})}{\\text{Pr}(y=-1|\\mathbf{x})} \\Big], \n",
    "\\end{equation}\n",
    "\n",
    "with the loss function\n",
    "\\begin{equation} \n",
    "    L(y,F) = \\text{log}(1+\\text{exp}(-2yF)), y \\in \\{-1,1\\}. (21)\n",
    "\\end{equation} \n",
    "\n",
    "* Calculate the derivative of log-likelihood in the __loop__. \n",
    "\n",
    "When class `BinomialDeviance` gets \\__call\\__, this will calculate the pseudo-response of $L$  \n",
    "\\begin{equation} \n",
    "    \\tilde{y}_i = - \\Big[\\frac{\\partial L\\big(y_i,F(\\mathbf{x_i})\\big)}{\\partial F(\\mathbf{x_i})} \\Big]_{F(\\mathbf{x}) = F_{m-1}(\\mathbf{x}) } = 2y_i/(1+\\text{exp}(2y_i F_{m-1}(\\mathbf{x}_i))).  (22)\n",
    "\\end{equation} \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Section 4.5 in [1]\n",
    "Two-class logistic regression and classification.\n",
    "\"\"\"            \n",
    "class BinomialDeviance(ClassificationLossFunction):\n",
    "    def init_estimator(self):\n",
    "        return LogOddsEstimator()\n",
    "\n",
    "    def __call__(self, y, pred, sample_weight=None):\n",
    "        \"\"\"Compute the deviance (= 2 * negative log-likelihood). \"\"\"\n",
    "        # logaddexp(0, v) == log(1.0 + exp(v))\n",
    "        pred = pred.ravel()\n",
    "        if sample_weight is None:\n",
    "            return -2.0 * np.mean((y * pred) - np.logaddexp(0.0, pred))\n",
    "        else:\n",
    "            return (-2.0 / sample_weight.sum() *\n",
    "                    np.sum(sample_weight * ((y * pred) - np.logaddexp(0.0, pred))))\n",
    "\n",
    "class LogOddsEstimator(object):\n",
    "    \"\"\"An estimator predicting the log odds ratio.\"\"\"\n",
    "    scale = 1.0\n",
    "\n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        # pre-cond: pos, neg are encoded as 1, 0\n",
    "        if sample_weight is None:\n",
    "            pos = np.sum(y)\n",
    "            neg = y.shape[0] - pos\n",
    "        else:\n",
    "            pos = np.sum(sample_weight * y)\n",
    "            neg = np.sum(sample_weight * (1 - y))\n",
    "\n",
    "        if neg == 0 or pos == 0:\n",
    "            raise ValueError('y contains non binary labels.')\n",
    "        self.prior = self.scale * np.log(pos / neg)\n",
    "\n",
    "    def predict(self, X):\n",
    "        check_is_fitted(self, 'prior')\n",
    "\n",
    "        y = np.empty((X.shape[0], 1), dtype=np.float64)\n",
    "        y.fill(self.prior)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `_fit_stages` will use loop many trees in order to iteratively, sequentially optimize the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseGradientBoosting(six.with_metaclass(ABCMeta, BaseEnsemble)):\n",
    "    def fit(self, X, y, sample_weight=None, monitor=None):\n",
    "        # fit the boosting stages\n",
    "        n_stages = self._fit_stages(X, y, y_pred, sample_weight, self._rng,\n",
    "                                    X_val, y_val, sample_weight_val,\n",
    "                                    begin_at_stage, monitor, X_idx_sorted)\n",
    "    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,\n",
    "                    X_val, y_val, sample_weight_val,\n",
    "                    begin_at_stage=0, monitor=None, X_idx_sorted=None):\n",
    "        \"\"\"Iteratively fits the stages.\n",
    "\n",
    "        For each stage it computes the progress (OOB, train score)\n",
    "        and delegates to ``_fit_stage``.\n",
    "        Returns the number of stages fit; might differ from ``n_estimators``\n",
    "        due to early stopping.\n",
    "        \"\"\"\n",
    "        # perform boosting iterations\n",
    "        i = begin_at_stage\n",
    "        for i in range(begin_at_stage, self.n_estimators):\n",
    "            # fit next stage of trees\n",
    "            y_pred = self._fit_stage(i, X, y, y_pred, sample_weight,\n",
    "                                     sample_mask, random_state, X_idx_sorted,\n",
    "                                     X_csc, X_csr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`_fit_stage` get called, this will work on just a single `DecisionTreeRegressor`. Each stage:\n",
    "* Calculate `loss.negative_gradient`, this wil be the `residual`, the error of previous learner.\n",
    "* Fit tree with the `residual`.\n",
    "* `update_terminal_regions` is to update node value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseGradientBoosting(six.with_metaclass(ABCMeta, BaseEnsemble)):\n",
    "    def _fit_stage(self, i, X, y, y_pred, sample_weight, sample_mask,\n",
    "                   random_state, X_idx_sorted, X_csc=None, X_csr=None):\n",
    "        \"\"\"Fit another stage of ``n_classes_`` trees to the boosting model. \"\"\"\n",
    "\n",
    "        loss = self.loss_\n",
    "        original_y = y\n",
    "\n",
    "        for k in range(loss.K):\n",
    "            if loss.is_multi_class:\n",
    "                y = np.array(original_y == k, dtype=np.float64)\n",
    "\n",
    "            residual = loss.negative_gradient(y, y_pred, k=k,\n",
    "                                              sample_weight=sample_weight)\n",
    "\n",
    "            # induce regression tree on residuals\n",
    "            tree = DecisionTreeRegressor()\n",
    "\n",
    "            if self.subsample < 1.0:\n",
    "                # no inplace multiplication!\n",
    "                sample_weight = sample_weight * sample_mask.astype(np.float64)\n",
    "\n",
    "            \n",
    "            tree.fit(X, residual, sample_weight=sample_weight,\n",
    "                         check_input=False, X_idx_sorted=X_idx_sorted)\n",
    "\n",
    "            # update tree leaves\n",
    "            loss.update_terminal_regions(tree.tree_, X, y, residual, y_pred,\n",
    "                                             sample_weight, sample_mask,\n",
    "                                             self.learning_rate, k=k)\n",
    "\n",
    "            # add tree to ensemble\n",
    "            self.estimators_[i, k] = tree\n",
    "\n",
    "        return y_pred\n",
    "    \n",
    "class LossFunction(six.with_metaclass(ABCMeta, object)):\n",
    "    def update_terminal_regions(self, tree, X, y, residual, y_pred,\n",
    "                                sample_weight, sample_mask,\n",
    "                                learning_rate=1.0, k=0):\n",
    "        # update each leaf (= perform line search)\n",
    "        for leaf in np.where(tree.children_left == TREE_LEAF)[0]:\n",
    "            self._update_terminal_region(tree, masked_terminal_regions,\n",
    "                                         leaf, X, y, residual,\n",
    "                                         y_pred[:, k], sample_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`loss.negative_gradient` and `loss.update_terminal_regions` will go to `negative_gradient` and `update_terminal_regions` respectively in  `LossFunction` class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossFunction(six.with_metaclass(ABCMeta, object)):\n",
    "    @abstractmethod\n",
    "    def negative_gradient(self, y, y_pred, **kargs):\n",
    "    \n",
    "    def update_terminal_regions(self, tree, X, y, residual, y_pred,\n",
    "                                sample_weight, sample_mask,\n",
    "                                learning_rate=1.0, k=0):\n",
    "\n",
    "    # update each leaf (= perform line search)\n",
    "        for leaf in np.where(tree.children_left == TREE_LEAF)[0]:\n",
    "            self._update_terminal_region(tree, masked_terminal_regions,\n",
    "                                         leaf, X, y, residual,\n",
    "                                         y_pred[:, k], sample_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With regression trees as base learners we again use stragegy M-regression tree in section 4.3 [1] of separate updates in each terminal node $R_{jm}$\n",
    "\\begin{equation}\n",
    "    \\gamma_{jm} = \\text{arg min}_{\\gamma} \\sum_{\\mathbf{x}_i \\in R_{jm}} \\text{log}(1 + \\text{exp}(-2y_i(F_{m-1}(\\mathbf{x}_i)+\\gamma))). (23)\n",
    "\\end{equation}\n",
    "\n",
    "[](\\begin{equation}\n",
    "    \\gamma_{jm} = \\sum_{\\mathbf{x}_i \\in R_{jm}} \\tilde{y}_i \\Bigg/\\sum_{\\mathbf{x}_i \\in R_{jm}} |\\tilde{y}_i|(2- |\\tilde{y}_i|).\n",
    "\\end{equation})\n",
    "\n",
    "The author stated there's no solution of (23). Following Jerome Friedman et al [2], Algorithm 6 to approximate it by a single Newton-Raphson step\n",
    "\\begin{equation}\n",
    "    \\gamma_{jm} = \\frac{y - p_j(\\mathbf{x})}{p_j(\\mathbf{x})(1-p_j(\\mathbf{x}))}. (23.1)\n",
    "\\end{equation}\n",
    "\n",
    "In **sklearn**, using `y - prob = residual`, (23.1) becomes `residual/(y - residual) * (1 - y + residual))`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinomialDeviance(ClassificationLossFunction):\n",
    "    def negative_gradient(self, y, pred, **kargs):\n",
    "        return y - expit(pred.ravel())\n",
    "\n",
    "    def _update_terminal_region(self, tree, terminal_regions, leaf, X, y, residual, pred, sample_weight):\n",
    "        \"\"\"Make a single Newton-Raphson step. Our node estimate is given by:\n",
    "            sum(w * (y - prob)) / sum(w * prob * (1 - prob))\n",
    "        we take advantage that: y - prob = residual\n",
    "        \"\"\"\n",
    "        terminal_region = np.where(terminal_regions == leaf)[0]\n",
    "        residual = residual.take(terminal_region, axis=0)\n",
    "        y = y.take(terminal_region, axis=0)\n",
    "        sample_weight = sample_weight.take(terminal_region, axis=0)\n",
    "\n",
    "        numerator = np.sum(sample_weight * residual)\n",
    "        denominator = np.sum(sample_weight * (y - residual) * (1 - y + residual))\n",
    "\n",
    "        if denominator == 0.0:\n",
    "            tree.value[leaf, 0, 0] = 0.0\n",
    "        else:\n",
    "            tree.value[leaf, 0, 0] = numerator / denominator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From [3]\n",
    "\n",
    "Let $g_i = g(\\mathbf{x_i})$, the model's loss function is measured by the error of the base learner $h(\\mathbf{x})$:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\begin{split}\n",
    "      L\\big(y,F(\\mathbf{x})\\big) &= \\sum^N_{i=1}[-g_i - h(\\mathbf{x_i})]^2  \\\\\n",
    "      &= \\sum^N_{i=1} [g^2_i + 2g_ih(\\mathbf{x_i}) + h^2(\\mathbf{x_i})]\\\\\n",
    "      &= \\sum^{T}_{j=1}[\\sum_{i\\in I_{j}}g^2_i + 2w_{j}\\sum_{i\\in I_{j}}g_i + n_{j}w^2_{j}]\n",
    "    \\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "where $n_{j}$ is the number of indices in region $R_j$. Let $I_j$ is the set of indices that belongs to region $R_j$, meaning $\\mathbf{x}_i \\in R_j \\text{ for } i \\in I_j$.  Let $G_j = \\sum_{i\\in I_{j}}g_i$, we further reduce to:\n",
    "\n",
    "\\begin{equation} \\label{eq.6.24}\n",
    "    L\\big(y,F(\\mathbf{x})\\big) = \\sum^{T}_{j=1}[ 2G_jw_{j} + n_{j}w^2_{j}] + \\text{constant}\n",
    "\\end{equation}\n",
    "\n",
    "With a fixed tree's structure, we compute the optimal weight $w\\ast$ and replace it to above:\n",
    "\\begin{equation} \\label{eq.6.25}\n",
    "    \\begin{split}\n",
    "        w\\ast &= -\\frac{G_j}{n_j} \\\\\n",
    "        L\\big(y,F(\\mathbf{x})\\big) &= -\\sum^{T}_{j=1}\\frac{G^2_j}{n_j} + \\text{constant}\n",
    "    \\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "From above we derive similarly to above, this is the proxy gain (ignore the other nodes and the normalization term) of a split is defined as:\n",
    "\\begin{equation}\n",
    "    \\begin{split}\n",
    "    \\mathcal{L}_{split} &= \\frac{G^2_L}{n_L} + \\frac{G^2_R}{n_R} - \\frac{(G_L + G_R)^2}{n_L + n_R} \\\\\n",
    "    &= \\frac{G^2_L}{n_L} + \\frac{G^2_R}{n_R} - \\frac{G^2}{n}\n",
    "    \\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "**sklearn** simplifies as\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{G^2_L}{n_L} + \\frac{G^2_R}{n_R}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdef class MSE(RegressionCriterion):\n",
    "    \"\"\"Mean squared error impurity criterion.\n",
    "\n",
    "        MSE = var_left + var_right\n",
    "    \"\"\"\n",
    "    cdef double proxy_impurity_improvement(self) nogil:\n",
    "        \"\"\"Compute a proxy of the impurity reduction\n",
    "        \n",
    "        This method is used to speed up the search for the best split.\n",
    "        It is a proxy quantity such that the split that maximizes this value\n",
    "        also maximizes the impurity improvement. It neglects all constant terms\n",
    "        of the impurity decrease for a given split.\n",
    "        \n",
    "        The absolute impurity improvement is only computed by the\n",
    "        impurity_improvement method once the best split has been found.\n",
    "        \"\"\"\n",
    "        # sum_left is the sum gradient from left node\n",
    "        cdef double* sum_left = self.sum_left\n",
    "        # sum_right is the sum gradient from right node\n",
    "        cdef double* sum_right = self.sum_right\n",
    "\n",
    "        cdef SIZE_t k\n",
    "        cdef double proxy_impurity_left = 0.0\n",
    "        cdef double proxy_impurity_right = 0.0\n",
    "\n",
    "        for k in range(self.n_outputs):\n",
    "            proxy_impurity_left += sum_left[k] * sum_left[k]\n",
    "            proxy_impurity_right += sum_right[k] * sum_right[k]\n",
    "\n",
    "        return (proxy_impurity_left / self.weighted_n_left +\n",
    "                proxy_impurity_right / self.weighted_n_right)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In [Sklearn document](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html) the `min_impurity_decrease` is mentionded as\n",
    "\n",
    "> A node will be split if this split induces a decrease of the impurity greater than or equal to this value.\n",
    "\n",
    "This is the absolute absolute impurity improvement of split after the best split above is found (mentioned in `proxy_impurity_improvement`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdef class Criterion:    \n",
    "    cdef double impurity_improvement(self, double impurity) nogil:\n",
    "        \"\"\"Compute the improvement in impurity\n",
    "\n",
    "        This method computes the improvement in impurity when a split occurs.\n",
    "        The weighted impurity improvement equation is the following:\n",
    "\n",
    "            N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
    "                                - N_t_L / N_t * left_impurity)\n",
    "\n",
    "        where N is the total number of samples, N_t is the number of samples\n",
    "        at the current node, N_t_L is the number of samples in the left child,\n",
    "        and N_t_R is the number of samples in the right child,\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        impurity : double\n",
    "            The initial impurity of the node before the split\n",
    "\n",
    "        Return\n",
    "        ------\n",
    "        double : improvement in impurity after the split occurs\n",
    "        \"\"\"\n",
    "\n",
    "        cdef double impurity_left\n",
    "        cdef double impurity_right\n",
    "\n",
    "        self.children_impurity(&impurity_left, &impurity_right)\n",
    "\n",
    "        return ((self.weighted_n_node_samples / self.weighted_n_samples) *\n",
    "                (impurity - (self.weighted_n_right / \n",
    "                             self.weighted_n_node_samples * impurity_right)\n",
    "                          - (self.weighted_n_left / \n",
    "                             self.weighted_n_node_samples * impurity_left)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Node impurity is computed as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdef class MSE(RegressionCriterion):    \n",
    "    cdef double node_impurity(self) nogil:\n",
    "        \"\"\"Evaluate the impurity of the current node, i.e. the impurity of\n",
    "           samples[start:end].\"\"\"\n",
    "\n",
    "        cdef double* sum_total = self.sum_total\n",
    "        cdef double impurity\n",
    "        cdef SIZE_t k\n",
    "\n",
    "        impurity = self.sq_sum_total / self.weighted_n_node_samples\n",
    "        for k in range(self.n_outputs):\n",
    "            impurity -= (sum_total[k] / self.weighted_n_node_samples)**2.0\n",
    "\n",
    "        return impurity / self.n_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When calling `GradientBoostingClassifier.predict` gets called, it comes down to\n",
    "* `_init_decision_function` calls to init `score`.\n",
    "* Each predict stage store the cumulative `score`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientBoostingClassifier(BaseGradientBoosting, ClassifierMixin):\n",
    "    def predict(self, X):\n",
    "        score = self.decision_function(X)\n",
    "        decisions = self.loss_._score_to_decision(score)\n",
    "        return self.classes_.take(decisions, axis=0)\n",
    "    \n",
    "    # This function will lead to the looping of sequentially optimize \n",
    "    # all the estimators_\n",
    "    def decision_function(self, X):\n",
    "        X = check_array(X, dtype=DTYPE, order=\"C\",  accept_sparse='csr')\n",
    "        score = self._decision_function(X)\n",
    "        if score.shape[1] == 1:\n",
    "            return score.ravel()\n",
    "        return score\n",
    "    \n",
    "    def _decision_function(self, X):\n",
    "        # for use in inner loop, not raveling the output in single-class case,\n",
    "        # not doing input validation.\n",
    "        score = self._init_decision_function(X)\n",
    "        predict_stages(self.estimators_, X, self.learning_rate, score)\n",
    "        return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`predict_stages` will be called in `_gradient_boosting.py`.\n",
    "Nodes are accessed by `estimators[i, k].tree_.nodes`, check `cdef class Tree` in `.tree._tree.pyx` to see all the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_stages(np.ndarray[object, ndim=2] estimators,\n",
    "                   object X, double scale,\n",
    "                   np.ndarray[float64, ndim=2] out):\n",
    "    for i in range(n_estimators):\n",
    "        for k in range(K):\n",
    "            tree = estimators[i, k].tree_\n",
    "\n",
    "            # avoid buffer validation by casting to ndarray\n",
    "            # and get data pointer\n",
    "            # need brackets because of casting operator priority\n",
    "            _predict_regression_tree_inplace_fast_dense(\n",
    "                <DTYPE_t*> (<np.ndarray> X).data,\n",
    "                tree.nodes, tree.value,\n",
    "                scale, k, K, X.shape[0], X.shape[1],\n",
    "                <float64 *> (<np.ndarray> out).data)\n",
    "            ## out += scale * tree.predict(X).reshape((X.shape[0], 1))\n",
    "\n",
    "cdef void _predict_regression_tree_inplace_fast_dense()\n",
    "    \"\"\"Predicts output for regression tree and stores it in ``out[i, k]``.\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : DTYPE_t pointer\n",
    "        The pointer to the data array of the input ``X``.\n",
    "        Assumes that the array is c-continuous.\n",
    "    root_node : tree Node pointer\n",
    "        Pointer to the main node array of the :class:``sklearn.tree.Tree``.\n",
    "    value : np.float64_t pointer\n",
    "        The pointer to the data array of the ``value`` array attribute\n",
    "        of the :class:``sklearn.tree.Tree``.\n",
    "    scale : double\n",
    "        A constant to scale the predictions.\n",
    "    \n",
    "    k : int\n",
    "        The index of the tree output to be predicted. Must satisfy\n",
    "        0 <= ``k`` < ``K``.\n",
    "    K : int\n",
    "        The number of regression tree outputs. For regression and\n",
    "        binary classification ``K == 1``, for multi-class\n",
    "        classification ``K == n_classes``.\n",
    "    n_samples : int\n",
    "        The number of samples in the input array ``X``;\n",
    "        ``n_samples == X.shape[0]``.\n",
    "    n_features : int\n",
    "        The number of features; ``n_samples == X.shape[1]``.\n",
    "    out : np.float64_t pointer\n",
    "        The pointer to the data array where the predictions are stored.\n",
    "        ``out`` is assumed to be a two-dimensional array of\n",
    "        shape ``(n_samples, K)``.\n",
    "    \"\"\"\n",
    "    cdef Py_ssize_t i\n",
    "    cdef Node *node\n",
    "    for i in range(n_samples):\n",
    "        node = root_node\n",
    "        # While node not a leaf\n",
    "        while node.left_child != TREE_LEAF:\n",
    "            if X[i * n_features + node.feature] <= node.threshold:\n",
    "                node = root_node + node.left_child\n",
    "            else:\n",
    "                node = root_node + node.right_child\n",
    "        out[i * K + k] += scale * value[node - root_node]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In summary\n",
    "\n",
    "## Fit\n",
    "1. Initialize\n",
    "    * Init base learner $h_0(\\mathbf{x})$.\n",
    "    * Init `estimators_` to store prediction of each learner for each feature (class), `shape = (n_estimators, n_classes)`.\n",
    "    * Init `train_score_` to store prediction after each loop, `shape = (n_estimators,)`\n",
    "    * Decide which loss function to use.\n",
    "    \n",
    "1. Calculate forward pass of learner $h_0(\\mathbf{x})$, this will be calculated only __1 time__.\n",
    "\n",
    "    `LogOddsEstimator` will calculate $F_0(\\mathbf{x})$\n",
    "\n",
    "    \\begin{equation}\n",
    "    F(\\mathbf{x}) = \\frac{1}{2} \\text{log} \\Big[ \\frac{\\text{Pr}(y=1|\\mathbf{x})}{\\text{Pr}(y=-1|\\mathbf{x})} \\Big], \n",
    "    \\end{equation}\n",
    "\n",
    "    with the loss function\n",
    "    \\begin{equation} \n",
    "        L(y,F) = \\text{log}(1+\\text{exp}(-2yF)), y \\in \\{-1,1\\}. (21)\n",
    "    \\end{equation} \n",
    "\n",
    "1. Calculate the derivative of log-likelihood in the __loop__. \n",
    "\n",
    "    When class `BinomialDeviance` gets \\__call\\__, this will calculate the pseudo-response of $L$  \n",
    "    \\begin{equation} \n",
    "        \\tilde{y}_i = - \\Big[\\frac{\\partial L\\big(y_i,F(\\mathbf{x_i})\\big)}{\\partial F(\\mathbf{x_i})} \\Big]_{F(\\mathbf{x}) = F_{m-1}(\\mathbf{x}) } = 2y_i/(1+\\text{exp}(2y_i F_{m-1}(\\mathbf{x}_i))).  (22)\n",
    "    \\end{equation} \n",
    "\n",
    "1. `_fit_stages` will use loop many trees in order to iteratively, sequentially optimize the tree.\n",
    "1. `_fit_stage` get called, this will work on just a single `DecisionTreeRegressor`. Each stage:\n",
    "\n",
    "    * Calculate `loss.negative_gradient`, this wil be the `residual`, the error of previous learner.\n",
    "    * Fit tree with the `residual`.\n",
    "    * `update_terminal_regions` (=leaves) of the given tree and updates the current predictions of the model.\n",
    "\n",
    "    `loss.negative_gradient` and `loss.update_terminal_regions` will go to `negative_gradient` and `update_terminal_regions` respectively in _class_ `LossFunction`. \n",
    "    \n",
    "    With regression trees as base learners we again use stragegy M-regression tree in section 4.3 of separate updates in each terminal node $R_{jm}$\n",
    "    \\begin{equation}\n",
    "        \\gamma_{jm} = \\text{arg min}_{\\gamma} \\sum_{\\mathbf{x}_i \\in R_{jm}} \\text{log}(1 + \\text{exp}(-2y_i(F_{m-1}(\\mathbf{x}_i)+\\gamma))). (23)\n",
    "    \\end{equation}\n",
    "\n",
    "    [](\\begin{equation}\n",
    "        \\gamma_{jm} = \\sum_{\\mathbf{x}_i \\in R_{jm}} \\tilde{y}_i \\Bigg/\\sum_{\\mathbf{x}_i \\in R_{jm}} |\\tilde{y}_i|(2- |\\tilde{y}_i|).\n",
    "    \\end{equation})\n",
    "\n",
    "    The author stated there's no solution of (23). Following Jerome Friedman et al [2], Algorithm 6 to approximate it by a single Newton-Raphson step\n",
    "    \\begin{equation}\n",
    "        \\gamma_{jm} = \\frac{y - p_j(\\mathbf{x})}{p_j(\\mathbf{x})(1-p_j(\\mathbf{x}))}. (23.1)\n",
    "    \\end{equation}\n",
    "\n",
    "    In **sklearn**, using `y - prob = residual`, (23.1) becomes `residual/(y - residual) * (1 - y + residual))`.\n",
    "    \n",
    "## Predict\n",
    "1. When calling `GradientBoostingClassifier.predict` gets called, it comes down to\n",
    "    * `_init_decision_function` calls to init `score`.\n",
    "    * Each predict stage store the cumulative `score`.\n",
    "\n",
    "    Nodes are accessed by `estimators[i, k].tree_.nodes`, check `cdef class Tree` in `.tree._tree.pyx` to see all the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossFunction(six.with_metaclass(ABCMeta, object)):\n",
    "    \"\"\"Abstract base class for various loss functions.\"\"\"\n",
    "    def __init__(self, n_classes):\n",
    "        self.K = n_classes\n",
    "    def init_estimator(self):\n",
    "        \"\"\"Init learner h_0(x)\"\"\"    \n",
    "    @abstractmethod\n",
    "    def __call__(self, y, pred, sample_weight=None):\n",
    "        \"\"\"Compute the loss of prediction ``pred`` and ``y``. \"\"\"\n",
    "    @abstractmethod\n",
    "    def negative_gradient(self, y, y_pred, **kargs):\n",
    "        \"\"\"Compute the negative gradient.\"\"\"\n",
    "    def update_terminal_regions():\n",
    "        \"\"\"Update the terminal regions (=leaves) of the given tree and\n",
    "        updates the current predictions of the model. Traverses tree\n",
    "        and invokes template method `_update_terminal_region`.\n",
    "        \n",
    "        # Compute leaf index for each sample in ``X``.\n",
    "        Point to `Tree.apply`\n",
    "        # Update each leaf (= perform line search)\n",
    "        For each leaf:\n",
    "            ``BinomialDeviance._update_terminal_region``\n",
    "        Update predictions of leaf value from leaf index above.    \n",
    "        \"\"\"\n",
    "            \n",
    "    @abstractmethod\n",
    "    def _update_terminal_region():\n",
    "\n",
    "class ClassificationLossFunction()\n",
    "    \"\"\"Base class for classification loss functions. \"\"\"\n",
    "    def _score_to_proba(self, score):\n",
    "        \"\"\"Template method to convert scores to probabilities.\n",
    "        \"\"\"\n",
    "    @abstractmethod\n",
    "    def _score_to_decision(self, score):\n",
    "        \"\"\"Point to _score_to_proba\n",
    "        \"\"\"    \n",
    "class LeastSquaresError(RegressionLossFunction):\n",
    "    \"\"\"Loss function for least squares (LS) estimation.\n",
    "    Terminal regions need not to be updated for least squares. \"\"\"\n",
    "    def update_terminal_regions():\n",
    "        \"\"\"Least squares does not need to update terminal regions.\n",
    "        But it has to update the predictions.\n",
    "        \"\"\"\n",
    "        # update predictions\n",
    "        y_pred[:, k] += learning_rate * tree.predict(X).ravel()\n",
    "        \n",
    "class BinomialDeviance(ClassificationLossFunction):\n",
    "    \"\"\"Binomial deviance loss function for binary classification.\n",
    "    Binary classification is a special case; here, we only need to\n",
    "    fit one tree instead of ``n_classes`` trees.\n",
    "    \"\"\"    \n",
    "    def __init__(self, n_classes):\n",
    "    def init_estimator(self):\n",
    "    def __call__(self, y, pred, sample_weight=None):\n",
    "    def negative_gradient(self, y, pred, **kargs):\n",
    "    def _update_terminal_region():\n",
    "        \"\"\"Make a single Newton-Raphson step.\n",
    "        our node estimate is given by:\n",
    "            sum(w * (y - prob)) / sum(w * prob * (1 - prob))\n",
    "        we take advantage that: y - prob = residual\n",
    "        \"\"\"        \n",
    "    def _score_to_proba(self, score):\n",
    "        \"\"\"Predict raw probability\"\"\"\n",
    "    def _score_to_decision(self, score):\n",
    "        \"\"\"Point to ``_score_to_proba``\n",
    "        Return index of max prob\n",
    "        \"\"\"\n",
    "        \n",
    "class BaseGradientBoosting()\n",
    "    # 5\n",
    "    def _fit_stage():\n",
    "        \"\"\"Fit another stage of ``n_classes_`` trees to the boosting model. \n",
    "        For each class:\n",
    "           Calculate ``residual = loss.negative_gradient``\n",
    "           Init ``DecisionTreeRegressor``\n",
    "           Fit ``Tree``\n",
    "           ``loss.update_terminal_regions`` (super(LossFunction)) or node values\n",
    "           Store the ``Tree`` to array \n",
    "        \"\"\"\n",
    "    # 3        \n",
    "    def _check_params():\n",
    "        \"\"\"Decide what loss function to use         \n",
    "        \"\"\"\n",
    "    # 2    \n",
    "    def _init_state():\n",
    "        \"\"\"Init base learner\n",
    "        Init array to store trees \n",
    "        Init array to store prediction \n",
    "        \"\"\"\n",
    "    def _clear_state():\n",
    "    def _resize_state():\n",
    "    def _is_initialized():\n",
    "    def _check_initialized():\n",
    "    # 1\n",
    "    def fit():\n",
    "        \"\"\"Init state\n",
    "        Fit stages \n",
    "        After early stoping update \n",
    "        \"\"\"\n",
    "    # 4    \n",
    "    def _fit_stages():\n",
    "        \"\"\"Sequentially optimize trees by fitting stage\n",
    "        For each stage:\n",
    "           Store prediction score \n",
    "           Monitor early stopping\n",
    "        \"\"\"\n",
    "    def _init_decision_function():    \n",
    "        \"\"\"Check input and compute prediction of ``init``. \"\"\"\n",
    "    def _decision_function(self, X):\n",
    "        \"\"\"Point to ``_init_decision_function``\n",
    "        Point to ``predict_stages``, calculate final prediction (all trees) \n",
    "        \"\"\"    \n",
    "    def _staged_decision_function(self, X):\n",
    "        \"\"\"Compute decision function of ``X`` for each iteration.\n",
    "        This method allows monitoring (i.e. determine error on testing set)\n",
    "        after each stage.\n",
    "        \n",
    "        Point to ``_init_decision_function``\n",
    "        Yield of `predict_stage` (single stage)\n",
    "        \"\"\"\n",
    "    def feature_importances_(self):\n",
    "    def _validate_y(self, y, sample_weight):\n",
    "    def apply(self, X):        \n",
    "        \"\"\"\n",
    "        For all trees in the ensemble to X, return leaf indices.\n",
    "        -------\n",
    "        X_leaves : array_like, shape = [n_samples, n_estimators]\n",
    "            For each datapoint x in X and for each tree in the ensemble,\n",
    "            return the index of the leaf x ends up in each estimator.\n",
    "        \n",
    "        For a single tree (recursively apply to a single Tree class)\n",
    "        Returns the index of the leaf that each sample is predicted as.\n",
    "        -------\n",
    "        X_leaves : array_like, shape = [n_samples,]\n",
    "            For each datapoint x in X, return the index of the leaf x\n",
    "            ends up in. Leaves are numbered within\n",
    "            ``[0; self.tree_.node_count)``, possibly with gaps in the\n",
    "            numbering.\n",
    "        \"\"\"\n",
    "        \n",
    "class GradientBoostingClassifier(BaseGradientBoosting):\n",
    "    def __init__():\n",
    "    def _validate_y(self, y, sample_weight):\n",
    "    def decision_function(self, X):\n",
    "    def staged_decision_function(self, X):\n",
    "    def predict(self, X):\n",
    "        \"\"\"Point to ``decision_function``\n",
    "        Point to ``loss_._score_to_decision``, choose max prob of each sample\n",
    "        Take the class has max prob\n",
    "        \"\"\"\n",
    "    def staged_predict(self, X):\n",
    "    def predict_proba(self, X):\n",
    "    def predict_log_proba(self, X):\n",
    "    def staged_predict_proba(self, X):\n",
    "\n",
    "class GradientBoostingRegressor(BaseGradientBoosting, RegressorMixin):\n",
    "    def __init__():    \n",
    "    def predict(self, X):\n",
    "    def staged_predict(self, X):\n",
    "    def apply(self, X):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FAQ\n",
    "Q: How to combine learner's prediction, i.e predict final output?\n",
    "\n",
    "A: Theoretically they say using major weight vote. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of Algorithm\n",
    "### V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv('data/cs-training.csv')\n",
    "df = df.drop('Unnamed: 0',axis=1)\n",
    "\n",
    "# Replace missing value with median\n",
    "df.MonthlyIncome = df.MonthlyIncome.fillna(df.MonthlyIncome.median())\n",
    "df.NumberOfDependents = df.NumberOfDependents.fillna(df.NumberOfDependents.median())\n",
    "X = df.drop([\"SeriousDlqin2yrs\"],axis=1)\n",
    "y = df[\"SeriousDlqin2yrs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((150000, 10), (150000,))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with small data\n",
    "X = X[1:10000]\n",
    "y = y[1:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "class GradientBoostClassifer():\n",
    "    def loss\n",
    "    def __init__(self, learning_rate = 1, n_estimators=10, debug = True):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self._learner = LogisticRegression()\n",
    "        self.learners_ = []\n",
    "        self._out = np.zeros((X.shape[0],1))\n",
    "        self.debug = debug\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        yi = y\n",
    "        for i in range(self.n_estimators):\n",
    "            # Fit each single weak learner\n",
    "            self._learner.fit(X,yi)\n",
    "            y_pred = self.learning_rate * self._learner.predict(X)\n",
    "            \n",
    "            # Error made by previous learner\n",
    "            residual = yi - y_pred\n",
    "            \n",
    "            # Assign the error to yi in order for next learner to fit in\n",
    "            yi = residual\n",
    "            \n",
    "            # Store all learners for later prediction\n",
    "            self.learners_.append(self._learner)\n",
    "            if self.debug is True:            \n",
    "                self._out += y_pred.reshape((X.shape[0], 1))\n",
    "                print(sum(y), sum(self._out)[0], sum(yi))\n",
    "\n",
    "    def predict(self, X):\n",
    "        n_trees = len(self.learners_)\n",
    "        out = np.zeros((X.shape[0],1))\n",
    "        for i in range(n_trees):\n",
    "            # Major hard vote\n",
    "            out += self.learning_rate * self.learners_[i].predict(X).reshape((X.shape[0], 1))\n",
    "#             if self.debug is True: print(sum(out)[0])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "639 38.0 601\n",
      "639 28.0 611\n",
      "639 50.0 589\n",
      "639 56.0 583\n",
      "639 80.0 559\n",
      "639 92.0 547\n",
      "639 116.0 523\n",
      "639 117.0 522\n",
      "639 139.0 500\n",
      "639 153.0 486\n",
      "140.0\n"
     ]
    }
   ],
   "source": [
    "clf = GradientBoostClassifer()\n",
    "clf.fit(X,y)\n",
    "y_pred = clf.predict(X)\n",
    "print(sum(y_pred)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "estimator = GradientBoostingClassifier(n_estimators=100)\n",
    "estimator.fit(X,y)\n",
    "y_pred = estimator.predict(X)\n",
    "print(sum(y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientBoostClassifer():\n",
    "    def __init__(self, learning_rate = 1, n_estimators=10, debug = True):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.debug = debug\n",
    "        \n",
    "    # Create arrays to store learners & score of each stage    \n",
    "    def _init_state(self):\n",
    "        self.estimators_ = np.empty((self.n_estimators,), dtype=np.object)\n",
    "        self.train_score_ = np.zeros((self.n_estimators,), dtype=np.float64)\n",
    "    \n",
    "    # We will use BinomialDeviance\n",
    "    def loss(y, y_pred):\n",
    "        y - y_pred\n",
    "        \n",
    "    def update_terminal_regions(self, tree, X, y, residual, \n",
    "                               y_pred, learning_rate):\n",
    "        \"\"\"\n",
    "        # Compute leaf index for each sample in ``X``.\n",
    "        Point to `Tree.apply`\n",
    "        # Update each leaf (= perform line search)\n",
    "        For each leaf:\n",
    "            ``BinomialDeviance._update_terminal_region``\n",
    "        Update predictions of leaf value from leaf index above.    \n",
    "        \"\"\"\n",
    "\n",
    "        terminal_regions_idx = tree.apply(X)\n",
    "        for leaf in np.where(tree.children_left == TREE_LEAF)[0]:\n",
    "            \n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self._init_state()\n",
    "        n_samples = X.shape[0]\n",
    "        \n",
    "        for i in range(self.n_estimators):\n",
    "            # Fit each single weak learner\n",
    "            self._learner.fit(X, yi)\n",
    "            y_pred = self.learning_rate * self._learner.predict(X)\n",
    "            \n",
    "            # Error made by previous learner\n",
    "            residual = loss(y, y_pred)\n",
    "            \n",
    "            # Update node value\n",
    "            update_terminal_region(tree.tree_, X, y, \n",
    "                                   residual, y_pred, learning_rate)\n",
    "            \n",
    "            # Assign the error to yi in order for next learner to fit in\n",
    "            yi = residual\n",
    "            \n",
    "            # Store all learners for later prediction\n",
    "            self.learners_.append(self._learner)\n",
    "            \n",
    "            if self.debug is True:            \n",
    "                self.train_score_[i] = loss(y, y_pred)\n",
    "                print(sum(y), sum(self._out)[0], sum(yi))\n",
    "\n",
    "    def predict(self, X):\n",
    "        n_trees = len(self.learners_)\n",
    "        out = np.zeros((X.shape[0],1))\n",
    "        for i in range(n_trees):\n",
    "            # Major hard vote\n",
    "            out += self.learning_rate * self.learners_[i].predict(X).reshape((X.shape[0], 1))\n",
    "        return out "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References & Readings\n",
    "[1] Greedy Function Approximation: A Gradient Boosting Machine. \n",
    "    Jerome H. Friedman*. IMS 1999 Reitz Lecture. \n",
    "    February 24,1999 (modified March 15, 2000, April 19, 2001)\n",
    "    \n",
    "[2] The Additive Logistic Regression: a Statistical View of. Boosting. Jerome Friedman. Trevor Hastie. Robert Tibshirani. y. July 23, 1998. (FHT00) [pdf](http://statweb.stanford.edu/~tibs/ftp/boost.ps)\n",
    "\n",
    "[3] https://towardsdatascience.com/boosting-algorithm-gbm-97737c63daa3\n",
    "\n",
    "[4] The Elements of Statistical Learning, February 2009. Trevor Hastie, Robert Tibshirani, Jerome H. Friedman. [pdf](https://web.stanford.edu/~hastie/Papers/ESLII.pdf)\n",
    "\n",
    "\n",
    "\n",
    "* https://stackoverflow.com/questions/4936788/decision-tree-learning-and-impurity\n",
    "\n",
    "In practical terms this means entropy measures (A) can't over-fit when used properly as they are free from any assumptions about the data, (B) are more likely to perform better than random because they generalize to any dataset but (C) the performance for specific datasets might not be as good as measures that adopt assumptions.\n",
    "\n",
    "When deciding which measures to use in machine learning it often comes down to long-term vs short-term gains, and maintainability. Entropy measures often work long-term by (A) and (B), and if something goes wrong it's easier to track down and explain why (e.g. a bug with obtaining the training data). Other approaches, by (C), might give short-term gains, but if they stop working it can be very hard to distinguish, say a bug in infrastructure with a genuine change in the data where the assumptions no longer hold.\n",
    "\n",
    "Unless you are implementing from scratch, most existing implementations use a single predetermined impurity measure. Note also that the Gini index is not a direct measure of impurity, not in its original formulation, and that there are many more than what you list above.\n",
    "\n",
    "* http://legacydirs.umiacs.umd.edu/~salzberg/docs/murthy_thesis/node15.html\n",
    "\n",
    "The goal of adding new nodes to a tree is to split up the sample space so as to minimize the **impurity** of the training set. Some algorithms measure **goodness** instead of impurity, the difference being that goodness values should be maximized while impurity should be minimized\n",
    "\n",
    "* http://www.cse.msu.edu/~cse802/DecisionTrees.pdf\n",
    "* https://www.analyticsvidhya.com/blog/2016/04/complete-tutorial-tree-based-modeling-scratch-in-python/\n",
    "\n",
    "* https://www.quora.com/What-is-the-difference-between-gradient-boosting-and-adaboost\n",
    "\n",
    "Both are boosting algorithms which means that they convert a set of weak learners into a single strong learner. They both initialize a strong learner (usually a decision tree) and iteratively create a weak learner that is added to the strong learner. They differ on how they create the weak learners during the iterative process.\n",
    "\n",
    "At each iteration, adaptive boosting changes the sample distribution by modifying the weights attached to each of the instances. It increases the weights of the wrongly predicted instances and decreases the ones of the correctly predicted instances. The weak learner thus focuses more on the difficult instances. After being trained, **the weak learner is added to the strong one according to his performance** (so-called alpha weight). The higher it performs, the more it contributes to the strong learner.\n",
    "\n",
    "On the other hand, gradient boosting doesn’t modify the sample distribution. Instead of training on a newly sample distribution, **the weak learner trains on the remaining errors** (so-called pseudo-residuals) of the strong learner. It is another way to give more importance to the difficult instances. At each iteration, the pseudo-residuals are computed and a weak learner is fitted to these pseudo-residuals. Then, the contribution of the weak learner (so-called multiplier) to **the strong one isn’t computed according to his performance on the newly distribution sample but using a gradient descent optimization process**. The computed contribution is the one minimizing the overall error of the strong learner.\n",
    "\n",
    "* Greatness of Tree ;-)\n",
    "\n",
    "They naturally incorporate mixtures of numeric and categorical predictor variables and missing values. They are invariant under (strictly monotone) transforma- tions of the individual predictors. As a result, scaling and/or more general transformations are not an issue, and they are immune to the effects of pre- dictor outliers. They perform internal feature selection as an integral part of the procedure. They are thereby resistant, if not completely immune, to the inclusion of many irrelevant predictor variables. These properties of decision trees are largely the reason that they have emerged as the most popular learning method for data mining.\n",
    "\n",
    "<img src=\"img/tb10-1.png\" width=\"400\" height=\"400\">\n",
    "\n",
    "* Loss functions\n",
    "\n",
    "<img src=\"img/tb10-2.png\" width=\"400\" height=\"400\">\n",
    "* Quantile Loss [Sklearn example](http://scikit-learn.org/stable/auto_examples/ensemble/plot_gradient_boosting_quantile.html) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
